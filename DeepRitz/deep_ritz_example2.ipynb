{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Ritz Method\n",
    "\n",
    "## Introduction of Example 2\n",
    "\n",
    "Using the Deep Ritz Method to solve the following PDE problem (Poisson Equation in High Dimension)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\Delta u(x) &= 0, &x \\in (0,1)^{10} \\\\\n",
    "u(x) &= \\sum_{k=1}^5x_{2k-1}x_{2k}, &x \\in \\partial(0,1)^{10}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Note:   \n",
    "> This code is from [xdfeng7370](https://github.com/xdfeng7370/Deep-Ritz-Method), some contents have been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, autograd\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define functions and Deep Ritz Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements simga(x)^(power)\n",
    "    Applies a power of the rectified linear unit element-wise.\n",
    "\n",
    "    NOTE: inplace may not be working.\n",
    "    Can set inplace for inplace operation if desired.\n",
    "    BUT I don't think it is working now.\n",
    "\n",
    "    INPUT:\n",
    "        x -- size (N, *) tensor where * is any number of additional dimensions\n",
    "    OUTPUT:\n",
    "        y -- size (N, *)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplace=False, power=3):\n",
    "        super(PowerReLU, self).__init__()\n",
    "        self.inplace = inplace\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = F.relu(input, inplace=self.inplace)\n",
    "        return torch.pow(y, self.power)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the block used in the Deep Ritz Paper,\n",
    "    it contains two fully-connected layers, activation function, and\n",
    "    a residual connection.\n",
    "\n",
    "    Parameters:\n",
    "        in_N  -- dimension of the input\n",
    "        width -- number of nodes in the interior middle layer\n",
    "        out_N -- dimension of the output\n",
    "        phi   -- activation function used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_N, width, out_N, phi=PowerReLU()):\n",
    "        super(Block, self).__init__()\n",
    "        # create the necessary linear layers\n",
    "        self.L1 = nn.Linear(in_N, width)\n",
    "        self.L2 = nn.Linear(width, out_N)\n",
    "        # choose appropriate activation function\n",
    "        self.phi = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.phi(self.L2(self.phi(self.L1(x)))) + x\n",
    "\n",
    "\n",
    "class drrnn(nn.Module):\n",
    "    \"\"\"\n",
    "    drrnn -- Deep Ritz Residual Neural Network\n",
    "\n",
    "    Implements a network with the architecture used in the deep ritz method paper\n",
    "\n",
    "    Parameters:\n",
    "        in_N  -- input dimension\n",
    "        out_N -- output dimension\n",
    "        m     -- width of layers that form blocks\n",
    "        depth -- number of blocks to be stacked\n",
    "        phi   -- the activation function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_N, m, out_N, depth=4, phi=PowerReLU()):\n",
    "        super(drrnn, self).__init__()\n",
    "        # set parameters\n",
    "        self.in_N = in_N\n",
    "        self.m = m\n",
    "        self.out_N = out_N\n",
    "        self.depth = depth\n",
    "        self.phi = nn.ReLU()\n",
    "        # list for holding all the blocks\n",
    "        self.stack = nn.ModuleList()\n",
    "\n",
    "        # add first layer to list\n",
    "        self.stack.append(nn.Linear(in_N, m))\n",
    "\n",
    "        # add middle blocks to list\n",
    "        for i in range(depth):\n",
    "            self.stack.append(Block(m, m, m))\n",
    "\n",
    "        # add output linear layer\n",
    "        self.stack.append(nn.Linear(m, out_N))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first layer\n",
    "        for i in range(len(self.stack)):\n",
    "            x = self.stack[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we randomly sample training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interior_points(N=1000, d=10):\n",
    "    \"\"\"\n",
    "    randomly sample N points from interior of [-1,1]^d\n",
    "    \"\"\"\n",
    "    return torch.rand(N, d) * 2 - 1\n",
    "\n",
    "\n",
    "def get_boundary_points(N=100):\n",
    "    \"\"\"\n",
    "    randomly sample points from boundaries\n",
    "    \"\"\"\n",
    "    xb = torch.rand(2 * 10 * N, 10)\n",
    "    for i in range(10):\n",
    "        xb[2 * i * N: (2 * i + 1) * N, i] = 0.\n",
    "        xb[(2 * i + 1) * N: (2 * i + 2) * N, i] = 1.\n",
    "\n",
    "    return xb\n",
    "\n",
    "\n",
    "def u(x):\n",
    "    u = 0\n",
    "    for i in range(5):\n",
    "        u += x[:,2*i:2*i+1] * x[:,2*i+1:2*i+2]\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight) # Initialize the weight Tensor with values using kaiming initialization.\n",
    "        nn.init.constant_(m.bias, 0.0) # Fill the bias Tensor with 0.0\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    epochs = 20000\n",
    "\n",
    "    in_N = 10\n",
    "    m = 10\n",
    "    out_N = 1\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = drrnn(in_N, m, out_N).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    criteon = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # x = torch.cat((xr, xb), dim=0)\n",
    "\n",
    "    # if 2 < m:\n",
    "    #     y = torch.zeros(x.shape[0], m - 2)\n",
    "    #     x = torch.cat((x, y), dim=1)\n",
    "    # # print(x.shape)\n",
    "    best_loss, best_epoch = 1000, 0\n",
    "    for epoch in range(epochs+1):\n",
    "\n",
    "        # generate the data set\n",
    "        xr = get_interior_points().to(device)\n",
    "        xr.requires_grad_() # we need to calculate the derivatives of xr, so we set required_grad = True\n",
    "        xb = get_boundary_points().to(device)\n",
    "\n",
    "        output_r = model(xr) # u(xr)\n",
    "        output_b = model(xb) # u(xb)\n",
    "        grads = autograd.grad(outputs=output_r, inputs=xr,\n",
    "                              grad_outputs=torch.ones_like(output_r),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        grads_sum = torch.sum(torch.pow(grads, 2), dim=1)\n",
    "        u1 = 0.5 * grads_sum\n",
    "        u1 = torch.mean(u1)\n",
    "        u2 = torch.mean(torch.pow(output_b-u(xb), 2))\n",
    "        loss = u1 + 20 * 500 * u2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('epoch:', epoch, 'loss:', loss.item(), 'loss_r:', u1.item(), 'loss_b:', (20* 500 * u2).item())\n",
    "            if epoch > int(4 * epochs / 5):\n",
    "                if torch.abs(loss) < best_loss:\n",
    "                    best_loss = loss.item()\n",
    "                    best_epoch = epoch\n",
    "                    torch.save(model.state_dict(), 'assets/weights/deep_ritz_example2_best.mdl')\n",
    "    print('best epoch:', best_epoch, 'best loss:', best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda3\\envs\\ai4s\\Lib\\site-packages\\torch\\autograd\\graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1216833.5 loss_r: 166.364990234375 loss_b: 1216667.125\n",
      "epoch: 100 loss: 9444.369140625 loss_r: 46.557254791259766 loss_b: 9397.8115234375\n",
      "epoch: 200 loss: 5679.72998046875 loss_r: 31.830673217773438 loss_b: 5647.8994140625\n",
      "epoch: 300 loss: 3653.927490234375 loss_r: 32.61579895019531 loss_b: 3621.311767578125\n",
      "epoch: 400 loss: 2526.60302734375 loss_r: 26.98415184020996 loss_b: 2499.618896484375\n",
      "epoch: 500 loss: 1786.626220703125 loss_r: 25.05324363708496 loss_b: 1761.572998046875\n",
      "epoch: 600 loss: 1452.3428955078125 loss_r: 22.45516014099121 loss_b: 1429.8876953125\n",
      "epoch: 700 loss: 1210.302490234375 loss_r: 23.71978187561035 loss_b: 1186.582763671875\n",
      "epoch: 800 loss: 1097.178466796875 loss_r: 22.216724395751953 loss_b: 1074.9617919921875\n",
      "epoch: 900 loss: 921.888916015625 loss_r: 24.267675399780273 loss_b: 897.6212158203125\n",
      "epoch: 1000 loss: 858.8582153320312 loss_r: 20.839433670043945 loss_b: 838.018798828125\n",
      "epoch: 1100 loss: 856.8363037109375 loss_r: 25.811494827270508 loss_b: 831.0247802734375\n",
      "epoch: 1200 loss: 760.1773071289062 loss_r: 23.563535690307617 loss_b: 736.61376953125\n",
      "epoch: 1300 loss: 719.0645141601562 loss_r: 21.6763858795166 loss_b: 697.3881225585938\n",
      "epoch: 1400 loss: 644.6912841796875 loss_r: 20.878204345703125 loss_b: 623.8130493164062\n",
      "epoch: 1500 loss: 620.4006958007812 loss_r: 21.618371963500977 loss_b: 598.7823486328125\n",
      "epoch: 1600 loss: 600.481689453125 loss_r: 20.86654281616211 loss_b: 579.6151733398438\n",
      "epoch: 1700 loss: 564.3214721679688 loss_r: 20.927473068237305 loss_b: 543.3939819335938\n",
      "epoch: 1800 loss: 515.207275390625 loss_r: 20.8524112701416 loss_b: 494.3548583984375\n",
      "epoch: 1900 loss: 490.63983154296875 loss_r: 20.157489776611328 loss_b: 470.4823303222656\n",
      "epoch: 2000 loss: 467.5956726074219 loss_r: 21.85927391052246 loss_b: 445.73638916015625\n",
      "epoch: 2100 loss: 475.35125732421875 loss_r: 19.629980087280273 loss_b: 455.7212829589844\n",
      "epoch: 2200 loss: 450.5242004394531 loss_r: 19.685077667236328 loss_b: 430.839111328125\n",
      "epoch: 2300 loss: 434.9476623535156 loss_r: 17.31053924560547 loss_b: 417.6371154785156\n",
      "epoch: 2400 loss: 401.9369812011719 loss_r: 19.036800384521484 loss_b: 382.9001770019531\n",
      "epoch: 2500 loss: 421.30926513671875 loss_r: 17.994495391845703 loss_b: 403.31475830078125\n",
      "epoch: 2600 loss: 405.03656005859375 loss_r: 16.011886596679688 loss_b: 389.0246887207031\n",
      "epoch: 2700 loss: 371.7374267578125 loss_r: 19.306377410888672 loss_b: 352.4310607910156\n",
      "epoch: 2800 loss: 402.8280334472656 loss_r: 18.69576644897461 loss_b: 384.13226318359375\n",
      "epoch: 2900 loss: 367.12457275390625 loss_r: 17.15477180480957 loss_b: 349.96978759765625\n",
      "epoch: 3000 loss: 386.81329345703125 loss_r: 14.478764533996582 loss_b: 372.33453369140625\n",
      "epoch: 3100 loss: 385.8839111328125 loss_r: 17.560672760009766 loss_b: 368.3232421875\n",
      "epoch: 3200 loss: 371.9064025878906 loss_r: 17.59371566772461 loss_b: 354.31268310546875\n",
      "epoch: 3300 loss: 364.7897033691406 loss_r: 13.886012077331543 loss_b: 350.9036865234375\n",
      "epoch: 3400 loss: 356.2852478027344 loss_r: 13.245772361755371 loss_b: 343.03948974609375\n",
      "epoch: 3500 loss: 325.9205017089844 loss_r: 15.794336318969727 loss_b: 310.12615966796875\n",
      "epoch: 3600 loss: 344.9209289550781 loss_r: 13.093914031982422 loss_b: 331.8270263671875\n",
      "epoch: 3700 loss: 333.08856201171875 loss_r: 13.72458267211914 loss_b: 319.3639831542969\n",
      "epoch: 3800 loss: 317.4373474121094 loss_r: 16.145174026489258 loss_b: 301.29217529296875\n",
      "epoch: 3900 loss: 333.5891418457031 loss_r: 14.746660232543945 loss_b: 318.84246826171875\n",
      "epoch: 4000 loss: 308.0987548828125 loss_r: 13.551877975463867 loss_b: 294.546875\n",
      "epoch: 4100 loss: 306.2037658691406 loss_r: 14.691752433776855 loss_b: 291.51202392578125\n",
      "epoch: 4200 loss: 317.5381774902344 loss_r: 14.857540130615234 loss_b: 302.6806335449219\n",
      "epoch: 4300 loss: 289.00726318359375 loss_r: 14.015840530395508 loss_b: 274.9914245605469\n",
      "epoch: 4400 loss: 287.92572021484375 loss_r: 13.487170219421387 loss_b: 274.43853759765625\n",
      "epoch: 4500 loss: 280.71490478515625 loss_r: 14.092002868652344 loss_b: 266.6228942871094\n",
      "epoch: 4600 loss: 278.56011962890625 loss_r: 13.83858585357666 loss_b: 264.7215270996094\n",
      "epoch: 4700 loss: 280.56219482421875 loss_r: 12.885685920715332 loss_b: 267.676513671875\n",
      "epoch: 4800 loss: 272.0444030761719 loss_r: 13.36359691619873 loss_b: 258.6808166503906\n",
      "epoch: 4900 loss: 265.8304443359375 loss_r: 13.155786514282227 loss_b: 252.67465209960938\n",
      "epoch: 5000 loss: 256.1628723144531 loss_r: 13.198736190795898 loss_b: 242.96414184570312\n",
      "epoch: 5100 loss: 254.56668090820312 loss_r: 13.651092529296875 loss_b: 240.91558837890625\n",
      "epoch: 5200 loss: 220.83372497558594 loss_r: 11.604472160339355 loss_b: 209.229248046875\n",
      "epoch: 5300 loss: 202.51829528808594 loss_r: 10.600080490112305 loss_b: 191.918212890625\n",
      "epoch: 5400 loss: 210.05239868164062 loss_r: 10.820232391357422 loss_b: 199.23216247558594\n",
      "epoch: 5500 loss: 204.1045379638672 loss_r: 11.816819190979004 loss_b: 192.2877197265625\n",
      "epoch: 5600 loss: 189.2117919921875 loss_r: 11.925445556640625 loss_b: 177.28634643554688\n",
      "epoch: 5700 loss: 179.04612731933594 loss_r: 11.195511817932129 loss_b: 167.85061645507812\n",
      "epoch: 5800 loss: 184.98934936523438 loss_r: 10.326754570007324 loss_b: 174.66259765625\n",
      "epoch: 5900 loss: 177.42408752441406 loss_r: 11.555841445922852 loss_b: 165.8682403564453\n",
      "epoch: 6000 loss: 165.44090270996094 loss_r: 11.255416870117188 loss_b: 154.18548583984375\n",
      "epoch: 6100 loss: 155.14102172851562 loss_r: 10.811834335327148 loss_b: 144.32919311523438\n",
      "epoch: 6200 loss: 145.35723876953125 loss_r: 11.02078914642334 loss_b: 134.33645629882812\n",
      "epoch: 6300 loss: 138.8419647216797 loss_r: 13.137639999389648 loss_b: 125.70433044433594\n",
      "epoch: 6400 loss: 132.67529296875 loss_r: 10.836052894592285 loss_b: 121.83924102783203\n",
      "epoch: 6500 loss: 133.06202697753906 loss_r: 11.65991497039795 loss_b: 121.40210723876953\n",
      "epoch: 6600 loss: 116.60325622558594 loss_r: 11.62735366821289 loss_b: 104.97590637207031\n",
      "epoch: 6700 loss: 115.81576538085938 loss_r: 10.178650856018066 loss_b: 105.63711547851562\n",
      "epoch: 6800 loss: 107.22062683105469 loss_r: 12.449491500854492 loss_b: 94.77113342285156\n",
      "epoch: 6900 loss: 108.469482421875 loss_r: 11.763100624084473 loss_b: 96.70638275146484\n",
      "epoch: 7000 loss: 100.21957397460938 loss_r: 11.866917610168457 loss_b: 88.35265350341797\n",
      "epoch: 7100 loss: 94.30535125732422 loss_r: 11.21982479095459 loss_b: 83.08552551269531\n",
      "epoch: 7200 loss: 95.16659545898438 loss_r: 10.69497299194336 loss_b: 84.47162628173828\n",
      "epoch: 7300 loss: 92.87014770507812 loss_r: 11.747772216796875 loss_b: 81.12237548828125\n",
      "epoch: 7400 loss: 92.78265380859375 loss_r: 10.503390312194824 loss_b: 82.27926635742188\n",
      "epoch: 7500 loss: 87.51126098632812 loss_r: 10.228755950927734 loss_b: 77.28250885009766\n",
      "epoch: 7600 loss: 78.99826049804688 loss_r: 10.785616874694824 loss_b: 68.212646484375\n",
      "epoch: 7700 loss: 85.30928802490234 loss_r: 9.825338363647461 loss_b: 75.48394775390625\n",
      "epoch: 7800 loss: 75.81144714355469 loss_r: 9.374894142150879 loss_b: 66.43655395507812\n",
      "epoch: 7900 loss: 72.70365905761719 loss_r: 9.307948112487793 loss_b: 63.39570999145508\n",
      "epoch: 8000 loss: 72.98107147216797 loss_r: 9.516121864318848 loss_b: 63.46495056152344\n",
      "epoch: 8100 loss: 68.93524169921875 loss_r: 8.688594818115234 loss_b: 60.24664306640625\n",
      "epoch: 8200 loss: 70.65886688232422 loss_r: 8.689013481140137 loss_b: 61.96985626220703\n",
      "epoch: 8300 loss: 67.56526947021484 loss_r: 9.023027420043945 loss_b: 58.542240142822266\n",
      "epoch: 8400 loss: 69.16094970703125 loss_r: 8.496824264526367 loss_b: 60.664127349853516\n",
      "epoch: 8500 loss: 63.15174102783203 loss_r: 8.213414192199707 loss_b: 54.93832778930664\n",
      "epoch: 8600 loss: 61.964351654052734 loss_r: 9.034117698669434 loss_b: 52.930233001708984\n",
      "epoch: 8700 loss: 62.86090087890625 loss_r: 8.44182014465332 loss_b: 54.41908264160156\n",
      "epoch: 8800 loss: 61.42971420288086 loss_r: 9.46451187133789 loss_b: 51.96520233154297\n",
      "epoch: 8900 loss: 57.39777755737305 loss_r: 7.983580589294434 loss_b: 49.4141960144043\n",
      "epoch: 9000 loss: 57.751041412353516 loss_r: 8.02475357055664 loss_b: 49.726287841796875\n",
      "epoch: 9100 loss: 55.3228759765625 loss_r: 6.88174295425415 loss_b: 48.441131591796875\n",
      "epoch: 9200 loss: 56.24128341674805 loss_r: 7.896915435791016 loss_b: 48.34436798095703\n",
      "epoch: 9300 loss: 52.99111557006836 loss_r: 7.548746585845947 loss_b: 45.44236755371094\n",
      "epoch: 9400 loss: 52.86727523803711 loss_r: 7.630784511566162 loss_b: 45.23649215698242\n",
      "epoch: 9500 loss: 51.644386291503906 loss_r: 6.757877349853516 loss_b: 44.88650894165039\n",
      "epoch: 9600 loss: 50.16386032104492 loss_r: 5.931857109069824 loss_b: 44.23200225830078\n",
      "epoch: 9700 loss: 48.29411315917969 loss_r: 5.554574012756348 loss_b: 42.739540100097656\n",
      "epoch: 9800 loss: 46.0186767578125 loss_r: 5.277318477630615 loss_b: 40.74135971069336\n",
      "epoch: 9900 loss: 44.60552215576172 loss_r: 4.570432662963867 loss_b: 40.03508758544922\n",
      "epoch: 10000 loss: 45.310508728027344 loss_r: 3.7738993167877197 loss_b: 41.5366096496582\n",
      "epoch: 10100 loss: 42.360267639160156 loss_r: 3.2329659461975098 loss_b: 39.12730026245117\n",
      "epoch: 10200 loss: 41.53316116333008 loss_r: 2.8992631435394287 loss_b: 38.6338996887207\n",
      "epoch: 10300 loss: 43.520835876464844 loss_r: 2.486614942550659 loss_b: 41.03422164916992\n",
      "epoch: 10400 loss: 42.1186408996582 loss_r: 2.031810760498047 loss_b: 40.086830139160156\n",
      "epoch: 10500 loss: 40.566619873046875 loss_r: 1.7911133766174316 loss_b: 38.77550506591797\n",
      "epoch: 10600 loss: 38.535186767578125 loss_r: 1.5459680557250977 loss_b: 36.989219665527344\n",
      "epoch: 10700 loss: 36.47011184692383 loss_r: 1.4132461547851562 loss_b: 35.05686569213867\n",
      "epoch: 10800 loss: 36.664432525634766 loss_r: 1.38966965675354 loss_b: 35.27476119995117\n",
      "epoch: 10900 loss: 36.09455490112305 loss_r: 1.3051953315734863 loss_b: 34.78936004638672\n",
      "epoch: 11000 loss: 36.036888122558594 loss_r: 1.1043530702590942 loss_b: 34.932533264160156\n",
      "epoch: 11100 loss: 33.32273864746094 loss_r: 1.1114710569381714 loss_b: 32.21126937866211\n",
      "epoch: 11200 loss: 33.01609420776367 loss_r: 1.233199954032898 loss_b: 31.782896041870117\n",
      "epoch: 11300 loss: 32.970829010009766 loss_r: 1.0937873125076294 loss_b: 31.87704086303711\n",
      "epoch: 11400 loss: 36.01404571533203 loss_r: 0.9694550037384033 loss_b: 35.04458999633789\n",
      "epoch: 11500 loss: 34.54900360107422 loss_r: 1.0686689615249634 loss_b: 33.4803352355957\n",
      "epoch: 11600 loss: 36.26813888549805 loss_r: 0.9676281213760376 loss_b: 35.30051040649414\n",
      "epoch: 11700 loss: 31.746572494506836 loss_r: 1.0328028202056885 loss_b: 30.713769912719727\n",
      "epoch: 11800 loss: 33.41533660888672 loss_r: 0.9464747309684753 loss_b: 32.4688606262207\n",
      "epoch: 11900 loss: 28.291257858276367 loss_r: 0.8369179964065552 loss_b: 27.4543399810791\n",
      "epoch: 12000 loss: 29.198028564453125 loss_r: 1.0039570331573486 loss_b: 28.19407081604004\n",
      "epoch: 12100 loss: 30.52610969543457 loss_r: 0.9995917081832886 loss_b: 29.526517868041992\n",
      "epoch: 12200 loss: 28.932159423828125 loss_r: 0.9864502549171448 loss_b: 27.945709228515625\n",
      "epoch: 12300 loss: 29.849748611450195 loss_r: 0.7972204089164734 loss_b: 29.052528381347656\n",
      "epoch: 12400 loss: 27.047592163085938 loss_r: 0.8942574262619019 loss_b: 26.153335571289062\n",
      "epoch: 12500 loss: 26.26774024963379 loss_r: 0.9314722418785095 loss_b: 25.336267471313477\n",
      "epoch: 12600 loss: 28.264114379882812 loss_r: 1.008252501487732 loss_b: 27.255861282348633\n",
      "epoch: 12700 loss: 27.954835891723633 loss_r: 0.951509952545166 loss_b: 27.003326416015625\n",
      "epoch: 12800 loss: 26.39396858215332 loss_r: 1.0068762302398682 loss_b: 25.38709259033203\n",
      "epoch: 12900 loss: 24.7841854095459 loss_r: 0.9337198734283447 loss_b: 23.850465774536133\n",
      "epoch: 13000 loss: 28.616697311401367 loss_r: 1.040528416633606 loss_b: 27.576168060302734\n",
      "epoch: 13100 loss: 32.07692337036133 loss_r: 1.1485886573791504 loss_b: 30.928335189819336\n",
      "epoch: 13200 loss: 26.008831024169922 loss_r: 1.1302505731582642 loss_b: 24.87858009338379\n",
      "epoch: 13300 loss: 24.760478973388672 loss_r: 1.1298264265060425 loss_b: 23.630653381347656\n",
      "epoch: 13400 loss: 26.29131507873535 loss_r: 1.3867329359054565 loss_b: 24.904582977294922\n",
      "epoch: 13500 loss: 25.040138244628906 loss_r: 1.1131062507629395 loss_b: 23.927032470703125\n",
      "epoch: 13600 loss: 23.752304077148438 loss_r: 1.0703299045562744 loss_b: 22.681974411010742\n",
      "epoch: 13700 loss: 25.666780471801758 loss_r: 1.1069694757461548 loss_b: 24.559810638427734\n",
      "epoch: 13800 loss: 24.19426918029785 loss_r: 1.2882883548736572 loss_b: 22.905981063842773\n",
      "epoch: 13900 loss: 24.505701065063477 loss_r: 1.2491071224212646 loss_b: 23.256593704223633\n",
      "epoch: 14000 loss: 23.33409881591797 loss_r: 1.187668800354004 loss_b: 22.14643096923828\n",
      "epoch: 14100 loss: 20.970046997070312 loss_r: 1.04690420627594 loss_b: 19.92314338684082\n",
      "epoch: 14200 loss: 23.760894775390625 loss_r: 1.1563844680786133 loss_b: 22.604509353637695\n",
      "epoch: 14300 loss: 29.489482879638672 loss_r: 1.1069293022155762 loss_b: 28.382553100585938\n",
      "epoch: 14400 loss: 22.449295043945312 loss_r: 1.3449113368988037 loss_b: 21.10438346862793\n",
      "epoch: 14500 loss: 22.831279754638672 loss_r: 1.0838100910186768 loss_b: 21.747468948364258\n",
      "epoch: 14600 loss: 21.6829891204834 loss_r: 1.1014329195022583 loss_b: 20.58155632019043\n",
      "epoch: 14700 loss: 21.369260787963867 loss_r: 0.9982273578643799 loss_b: 20.37103271484375\n",
      "epoch: 14800 loss: 21.954355239868164 loss_r: 0.9810847043991089 loss_b: 20.973270416259766\n",
      "epoch: 14900 loss: 21.633949279785156 loss_r: 1.0097285509109497 loss_b: 20.62421989440918\n",
      "epoch: 15000 loss: 21.478071212768555 loss_r: 0.9607182145118713 loss_b: 20.517353057861328\n",
      "epoch: 15100 loss: 21.249340057373047 loss_r: 1.0010508298873901 loss_b: 20.248289108276367\n",
      "epoch: 15200 loss: 21.261550903320312 loss_r: 0.8405424952507019 loss_b: 20.421009063720703\n",
      "epoch: 15300 loss: 27.28130340576172 loss_r: 0.9263961911201477 loss_b: 26.354907989501953\n",
      "epoch: 15400 loss: 20.16652488708496 loss_r: 0.8742954730987549 loss_b: 19.29222869873047\n",
      "epoch: 15500 loss: 27.064849853515625 loss_r: 1.0094794034957886 loss_b: 26.055370330810547\n",
      "epoch: 15600 loss: 19.37348175048828 loss_r: 0.8325884342193604 loss_b: 18.5408935546875\n",
      "epoch: 15700 loss: 19.56747817993164 loss_r: 0.8560072183609009 loss_b: 18.711471557617188\n",
      "epoch: 15800 loss: 18.97257423400879 loss_r: 0.7703636288642883 loss_b: 18.202211380004883\n",
      "epoch: 15900 loss: 25.102901458740234 loss_r: 0.7362582087516785 loss_b: 24.36664390563965\n",
      "epoch: 16000 loss: 18.94091033935547 loss_r: 0.7893781065940857 loss_b: 18.151533126831055\n",
      "epoch: 16100 loss: 19.973743438720703 loss_r: 0.7672730684280396 loss_b: 19.206470489501953\n",
      "epoch: 16200 loss: 18.23572540283203 loss_r: 0.7314889430999756 loss_b: 17.504236221313477\n",
      "epoch: 16300 loss: 19.622392654418945 loss_r: 0.7883606553077698 loss_b: 18.83403205871582\n",
      "epoch: 16400 loss: 21.136747360229492 loss_r: 0.7911404967308044 loss_b: 20.34560775756836\n",
      "epoch: 16500 loss: 17.192476272583008 loss_r: 0.8149824738502502 loss_b: 16.377492904663086\n",
      "epoch: 16600 loss: 18.14378547668457 loss_r: 0.8387356400489807 loss_b: 17.305049896240234\n",
      "epoch: 16700 loss: 17.205320358276367 loss_r: 0.7560999989509583 loss_b: 16.449220657348633\n",
      "epoch: 16800 loss: 22.679346084594727 loss_r: 0.7622359395027161 loss_b: 21.917110443115234\n",
      "epoch: 16900 loss: 16.570940017700195 loss_r: 0.6692190170288086 loss_b: 15.901721000671387\n",
      "epoch: 17000 loss: 18.514497756958008 loss_r: 0.7873817682266235 loss_b: 17.727115631103516\n",
      "epoch: 17100 loss: 16.987165451049805 loss_r: 0.7542070150375366 loss_b: 16.23295783996582\n",
      "epoch: 17200 loss: 16.651742935180664 loss_r: 0.7364744544029236 loss_b: 15.915267944335938\n",
      "epoch: 17300 loss: 16.61019515991211 loss_r: 0.7715275287628174 loss_b: 15.838667869567871\n",
      "epoch: 17400 loss: 18.302818298339844 loss_r: 0.7437724471092224 loss_b: 17.559045791625977\n",
      "epoch: 17500 loss: 15.89752197265625 loss_r: 0.6961817741394043 loss_b: 15.201339721679688\n",
      "epoch: 17600 loss: 17.008230209350586 loss_r: 0.8091245293617249 loss_b: 16.199106216430664\n",
      "epoch: 17700 loss: 14.882753372192383 loss_r: 0.775435745716095 loss_b: 14.107317924499512\n",
      "epoch: 17800 loss: 14.997232437133789 loss_r: 0.7573176026344299 loss_b: 14.239914894104004\n",
      "epoch: 17900 loss: 15.485862731933594 loss_r: 0.7204833030700684 loss_b: 14.765379905700684\n",
      "epoch: 18000 loss: 17.2728271484375 loss_r: 0.716113805770874 loss_b: 16.556713104248047\n",
      "epoch: 18100 loss: 15.973289489746094 loss_r: 0.7248600125312805 loss_b: 15.248429298400879\n",
      "epoch: 18200 loss: 15.368520736694336 loss_r: 0.7805945873260498 loss_b: 14.587925910949707\n",
      "epoch: 18300 loss: 14.682560920715332 loss_r: 0.6858586668968201 loss_b: 13.996702194213867\n",
      "epoch: 18400 loss: 15.69625186920166 loss_r: 0.7265611886978149 loss_b: 14.969690322875977\n",
      "epoch: 18500 loss: 14.3988618850708 loss_r: 0.7573840022087097 loss_b: 13.641477584838867\n",
      "epoch: 18600 loss: 15.437143325805664 loss_r: 0.753613293170929 loss_b: 14.6835298538208\n",
      "epoch: 18700 loss: 14.66358757019043 loss_r: 0.6793451309204102 loss_b: 13.98424243927002\n",
      "epoch: 18800 loss: 17.317615509033203 loss_r: 0.7731478214263916 loss_b: 16.54446792602539\n",
      "epoch: 18900 loss: 14.729792594909668 loss_r: 0.6739460825920105 loss_b: 14.055846214294434\n",
      "epoch: 19000 loss: 14.891907691955566 loss_r: 0.8256444931030273 loss_b: 14.066263198852539\n",
      "epoch: 19100 loss: 14.452857971191406 loss_r: 0.6485273838043213 loss_b: 13.804330825805664\n",
      "epoch: 19200 loss: 14.521099090576172 loss_r: 0.6977607607841492 loss_b: 13.823338508605957\n",
      "epoch: 19300 loss: 17.14053726196289 loss_r: 0.6311489343643188 loss_b: 16.509387969970703\n",
      "epoch: 19400 loss: 14.560164451599121 loss_r: 0.6724636554718018 loss_b: 13.887701034545898\n",
      "epoch: 19500 loss: 14.231656074523926 loss_r: 0.6345961093902588 loss_b: 13.597060203552246\n",
      "epoch: 19600 loss: 15.617849349975586 loss_r: 0.6308807730674744 loss_b: 14.986968994140625\n",
      "epoch: 19700 loss: 14.189251899719238 loss_r: 0.6517216563224792 loss_b: 13.537529945373535\n",
      "epoch: 19800 loss: 13.289979934692383 loss_r: 0.5760979056358337 loss_b: 12.713882446289062\n",
      "epoch: 19900 loss: 13.970354080200195 loss_r: 0.5543038845062256 loss_b: 13.41604995727539\n",
      "epoch: 20000 loss: 13.904017448425293 loss_r: 0.5526555776596069 loss_b: 13.351362228393555\n",
      "best epoch: 19800 best loss: 13.289979934692383\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weights from ckpt!\n",
      "L^2 relative error: 0.026047656312584877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\43757\\AppData\\Local\\Temp\\ipykernel_29636\\2286512218.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('assets/weights/deep_ritz_example2_best.mdl'))\n"
     ]
    }
   ],
   "source": [
    "# plot figure\n",
    "in_N = 10\n",
    "m = 10\n",
    "out_N = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = drrnn(in_N, m, out_N).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('assets/weights/deep_ritz_example2_best.mdl'))\n",
    "print('load weights from ckpt!')\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.rand(100000,10)\n",
    "    x = x.to(device)\n",
    "    u_exact = u(x)\n",
    "    u_pred = model(x)\n",
    "err_l2 = torch.sqrt(torch.mean(torch.pow(u_pred-u_exact,2))) / torch.sqrt(torch.mean(torch.pow(u_exact,2)))\n",
    "print('L^2 relative error:', err_l2.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems](http://link.springer.com/10.1007/s40304-018-0127-z)\n",
    "2. [Deep-Ritz-Method GitHub](https://github.com/xdfeng7370/Deep-Ritz-Method)\n",
    "3. [DeepRitz GitHub](https://github.com/junbinhuang/DeepRitz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
